---
layout: post
title: "[ML03-04] 모두를 위한 딥러닝: 기본적인 머신러닝과 딥러닝 강좌 정리"
date: 2020-08-24 00:00:00
categories: [SOPHOMORE]
tags: [MACHINE_LEARNING]
last_modified_at: 2020-08-24
---

<p>
From Inflearn
<br>By Sung Kim(김성훈)
<br>With TensorFlow v1</p>

[Colab_ML03](https://colab.research.google.com/drive/1gwdT0_1IjHiZQZPVARZXXPcCApPIRBk3#scrollTo=ZklcZx_YAJZV&uniqifier=1)
<br>
[Colab_ML04](https://colab.research.google.com/drive/1wXYHoenvknPa1pz4QXdrSqXLZ30QMznm)

---

__Hypothesis and Cost__
<p>
Cost(W,b) = E( square(H(X)-Y) ) (X = [x], Y = [y] : training set)
<br> * 이때 여러 H(x)중에서 cost를 최소값으로 가지는 가설을 선택
<br> * 그래프에서 W값이 x축, cost(W,b)가 y축에 위치함.
</p>

###### Gradient Descent Algorithm
<p>
: 임의의 W점에서의 기울기를 통해, W값의 증가/감소가 결정.
결론적으로 W를 움직여서 가장 기울기가 작은 곳(0에 수렴하는 W값)을 찾아가는 알고리즘
</p>

<figure>
  <center><img src="/Fortune/assets/ML/GradientDescentAlgorithm_W.png" alt="Midnight" style="width:100%"></center>
</figure>

<p>
* 위의 수식에서 W에서 빼고 있는 값은 LearningRate(α) * Cost'(Cost의 기울기)를 의미
<br> * Cost의 기울기가 양수이면, 다음 W값이 작아져 W은 왼쪽으로 이동하며
<br> &emsp;기울기가 음수이면, 다음 W값이 커져 W는 오른쪽으로 이동함
<br> * 원래 미분을 하면 2를 더 나누는데, 계산에는 영향을 미치지 않으므로 간략화됨.
</p>

> Gradient Descent Algorithm으로 Linear Regression을 진행할 때,
Cost Function은 Convex Function(볼록 함수)여야 함.

<br>

__Multi-Variable Linear Regression__
<p>
H(x) =  Wx + b 일때는 x, 즉 변수가 한개.
<br>그러나 위의 이름과 같이 변수가 여러개인 상황에서는 Matrix의 곱셈을 이용하여 간략화 함.
<br>따라서 행렬의 곱의 특징을 이해하는 것이 좋음
</p>

<figure>
  <center><img src="/Fortune/assets/ML/H_usingMatrix.png" alt="Midnight" style="width:100%"></center>
</figure>

<p>
* X 행렬을 [a, b]라고 표현할 때, a는 instance의 개수, b는 variable의 개수를 의미.
<br> * W 행렬을 [b, c]라고 표현할 때, b는 위와 같이 변수의 개수, c는 가설의 X에 대한 Y값의 개수를 의미함.(Linear에서는 하나밖에 없음)
<br> * X행렬과 W행렬의 곱이므로, H 행렬은 [a, c]라고 표현할 수 있음.
</p>

<figure>
  <center><img src="/Fortune/assets/ML/WX_XW.png" alt="Midnight" style="width:100%"></center>
</figure>

<p>
<br> Matrix를 이용하면, W와 X의 순서가 바뀌는 것을 볼 수 있음. 그래서 H(X) = XW라고 표기하면 이는 행렬을 의미함. 결론적으로는 가설이라는 같은 의미를 가짐.
</p>

<br>
<br>



