---
layout: post
title: "[MACHINE_LEARNING] Gradient Descent Algorithm and Multi-Variable Linear Regression"
date: 2020-08-24 00:00:00
categories: [STUDY]
tags: [MACHINE_LEARNING]
last_modified_at: 2020-08-24
---

<p>
From 모두를 위한 딥러닝
<br>By Sung Kim(김성훈)
<br>With TensorFlow</p>

[Colab_ML03](https://github.com/Sinyoung3016/MachineLearning_Tutorial/blob/master/ML_lab03.ipynb)
<br>
[Colab_ML04](https://github.com/Sinyoung3016/MachineLearning_Tutorial/blob/master/ML_lab04.ipynb)

---


__Hypothesis and Cost__

<p>
Cost(W,b) = E( square(H(X)-Y) ) (X = [x], Y = [y] : training set)
<br> * 이때 여러 H(x)중에서 cost를 최소값으로 가지는 가설을 선택
<br> * 그래프에서 W값이 x축, cost(W,b)가 y축에 위치함.
</p>

<br>

###### Gradient Descent Algorithm

<p>
: 임의의 W점에서의 기울기를 통해, W값의 증가/감소가 결정.
결론적으로 W를 움직여서 가장 기울기가 작은 곳(0에 수렴하는 W값)을 찾아가는 알고리즘
</p>

<figure>
  <center><img src="/Fortune/assets/Python/ML/0304/GradientDescentAlgorithm_W.png" alt="Midnight" style="width:100%"></center>
</figure>

<p>
* 위의 수식에서 W에서 빼고 있는 값은 LearningRate(α) * Cost'(Cost의 기울기)를 의미
<br> * Cost의 기울기가 양수이면, 다음 W값이 작아져 W은 왼쪽으로 이동하며
<br> &emsp;기울기가 음수이면, 다음 W값이 커져 W는 오른쪽으로 이동함
<br> * 원래 미분을 하면 2를 더 나누는데, 계산에는 영향을 미치지 않으므로 간략화됨.
</p>

> Gradient Descent Algorithm으로 Linear Regression을 진행할 때,
Cost Function은 Convex Function(볼록 함수)여야 함.

<br>

__Code__

```{.python}
##Graph 그리기

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

#1.data set 설정
x_train = [1, 2, 3, 4]
y_train = [1, 2, 3, 4]

#2.Model 구성
tf.model = tf.keras.Sequential()
  # units == output shape, input_dim == input shape
tf.model.add(tf.keras.layers.Dense(units=1, input_dim=1))

#3. Model 학습과정 설정
  # SGD == standard gradient descendent, lr == learning rate
sgd = tf.keras.optimizers.SGD(lr=0.1)
  # mse == mean_squared_error, 1/m * sig (y'-y)^2
tf.model.compile(loss='mse', optimizer=sgd)
  # prints summary of the model to the terminal
tf.model.summary()

#4. Model 학습
  # fit() trains the model and returns history of train
history = tf.model.fit(x_train, y_train, epochs=100)

#6. Model 사용하기
  # predict() returns predicted value
y_predict = tf.model.predict(np.array([5, 4]))
print(y_predict)

  # Plot training & validation loss values
plt.plot(history.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

```

<br>

__Result__

<figure>
  <center><img src="/Fortune/assets/Python/ML/0304/3Code1.png" alt="Midnight" style="width:100%"></center>
</figure>
<figure>
  <center><img src="/Fortune/assets/Python/ML/0304/3Code2.png" alt="Midnight" style="width:100%"></center>
</figure>

<br>

__Multi-Variable Linear Regression__

<p>
H(x) =  Wx + b 일때는 x, 즉 변수가 한개.
<br>그러나 위의 이름과 같이 변수가 여러개인 상황에서는 Matrix의 곱셈을 이용하여 간략화 함.
<br>따라서 행렬의 곱의 특징을 이해하는 것이 좋음
</p>

<figure>
  <center><img src="/Fortune/assets/Python/ML/0304/H_usingMatrix.png" alt="Midnight" style="width:100%"></center>
</figure>

<p>
* X 행렬을 [a, b]라고 표현할 때, a는 instance의 개수, b는 variable의 개수를 의미.
<br> * W 행렬을 [b, c]라고 표현할 때, b는 위와 같이 변수의 개수, c는 가설의 X에 대한 Y값의 개수를 의미함.(Linear에서는 하나밖에 없음)
<br> * X행렬과 W행렬의 곱이므로, H 행렬은 [a, c]라고 표현할 수 있음.
</p>

<figure>
  <center><img src="/Fortune/assets/Python/ML/0304/WX_XW.png" alt="Midnight" style="width:100%"></center>
</figure>

<p>
Matrix를 이용하면, W와 X의 순서가 바뀌는 것을 볼 수 있음. 그래서 H(X) = XW라고 표기하면 이는 행렬을 의미함. 결론적으로는 가설이라는 같은 의미를 가짐.
</p>

<br>

__Code__

```{.python}

##multi-variable regression

import tensorflow as tf
import numpy as np

#1.dataSet 설정
x_data = [[73., 80., 75.], [93., 88., 93.], [89., 91., 90.], [96., 98., 100.], [73., 66., 70.]]
y_data = [[152.], [185.], [180.], [196.], [142.]]

#2.Model 구성
tf.model = tf.keras.Sequential()
  # input_dim=3 gives multi-variable regression
tf.model.add(tf.keras.layers.Dense(units=1, input_dim=3))
  # linear activation is default
tf.model.add(tf.keras.layers.Activation('linear'))

#3. Model 학습과정 설정
tf.model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-5))

#4. Model 출력
tf.model.summary()

#5. Model 학습
history = tf.model.fit(x_data, y_data, epochs=100)

#6. Model 사용하기
y_predict = tf.model.predict(np.array([[72., 93., 90.],[60., 80., 80]]))
print(y_predict)

```

<br>

__Result__

<figure>
  <center><img src="/Fortune/assets/Python/ML/0304/04Code1.png" alt="Midnight" style="width:100%"></center>
</figure>
<figure>
  <center><img src="/Fortune/assets/Python/ML/0304/04Code2.png" alt="Midnight" style="width:100%"></center>
</figure>



<br>
<br>
<br>




