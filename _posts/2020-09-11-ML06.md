---
layout: post
title: "[MACHINE_LEARNING] Multinomial Classification"
date: 2020-09-11 00:00:00
categories: [STUDY]
tags: [MACHINE_LEARNING]
last_modified_at: 2020-09-11
---

<p>
From 모두를 위한 딥러닝
<br>By Sung Kim(김성훈)
<br>With TensorFlow</p>

[Colab_ML06](https://github.com/Sinyoung3016/MachineLearning_Tutorial/blob/master/ML_lab06.ipynb)

---

__Logistic Classification__

###### Multinomial Classification
<p>
: 여러개의 선택지 중 하나를 선택하는 결과를 가짐.
</p>

<figure>
  <center><img src="/Fortune/assets/Python/ML/0506/multi_graph.jpg" alt="Midnight" style="width:100%"></center>
</figure>

<p>
위와 같이 세가지로 분류할 수 있다고 가정할 때, 3가지 Binary Classification을 진행하여 Multinomial Classsification을 구현.
</p>

<br>

__Hypothesis__

<figure>
<center><img src="/Fortune/assets/Python/ML/0506/multi_hypo.jpg" alt="Midnight" style="width:100%"></center>
</figure>

<p>
* 세가지 가설을 행렬을 사용하여 간략화
<br>* binary는 sigmoid를 사용했지만, 여기서는 softmax 함수를 이용해 예측값을 출력(probabilities).
<br>* softmax의 두가지 특징 : 0부터 1사이의 값이며, 모든 출력값의 합이 1.
<br>* One-Hot-Encoding : array내 가장 큰 값을 출력하는 것으로, 1또는 0으로 이루어짐.
<br>* 결국 Y_data랑 같은 값임.
</p>

<br>

__Cost__

<figure>
  <center><img src="/Fortune/assets/Python/ML/0506/multi_cost.jpg" alt="Midnight" style="width:100%"></center>
  <figcaption>Cross-Entropy</figcaption>
</figure>

<p>
Cross-Entropy
* 결론적으로 Logistic cost 함수와 같음.
<br>* Cost 최소화는 Gradient Descent Optimizer를 사용
</p>

<br>

__Code__

```{.python}

## multinominal classification
import tensorflow as tf
import numpy as np

#1.dataSet 설정
x_raw = [[1, 2, 1, 1],
         [2, 1, 3, 2],
         [3, 1, 3, 4],
         [4, 1, 5, 5],
         [1, 7, 5, 5],
         [1, 2, 5, 6],
         [1, 6, 6, 6],
         [1, 7, 7, 7]]
y_raw = [[0, 0, 1],
         [0, 0, 1],
         [0, 0, 1],
         [0, 1, 0],
         [0, 1, 0],
         [0, 1, 0],
         [1, 0, 0],
         [1, 0, 0]]
         #One Hot Encoding

x_data = np.array(x_raw, dtype=np.float32)
y_data = np.array(y_raw, dtype=np.float32)

nb_classes = 3

#2. Model 구성
tf.model = tf.keras.Sequential()
tf.model.add(tf.keras.layers.Dense(input_dim=4, units=nb_classes, use_bias=True))
  # use softmax activations: softmax = exp(logits) / reduce_sum(exp(logits), dim)
tf.model.add(tf.keras.layers.Activation('softmax'))

#3. Model 학습과정 설정
# use loss == categorical_crossentropy
tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(lr=0.1), metrics=['accuracy'])

#4. Model 출력
tf.model.summary()

#5. Model 학습
history = tf.model.fit(x_data, y_data, epochs=1000)

#6. Model 사용
print('--------------')
#출력 : One-hot encoding
# argmax : array에서 가장 큰값을 출력
a = tf.model.predict(np.array([[1, 11, 7, 9]]))
print(a, tf.keras.backend.eval(tf.argmax(a, axis=1)))

print('--------------')
# or use argmax embedded method, predict_classes
c = tf.model.predict(np.array([[1, 1, 0, 1]]))
c_onehot = tf.model.predict_classes(np.array([[1, 1, 0, 1]]))
print(c, c_onehot)

print('--------------')
all = tf.model.predict(np.array([[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]))
all_onehot = tf.model.predict_classes(np.array([[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]))
print(all, all_onehot)

```

<br>

__Result__

<figure>
  <center><img src="/Fortune/assets/Python/ML/0506/code03.png" alt="Midnight" style="width:100%"></center>
</figure>

<figure>
  <center><img src="/Fortune/assets/Python/ML/0506/code04.png" alt="Midnight" style="width:100%"></center>
</figure>


<br>
<br>



